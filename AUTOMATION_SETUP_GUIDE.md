# ü§ñ Automation Setup Guide

Complete guide to setting up and using the autonomous keyword research features.

---

## ‚úÖ What's Now Automated

Your keyword research tool now includes **5 powerful automation systems**:

1. **Autonomous Seed Discovery** - Auto-find keywords from URLs/competitors
2. **Scheduled Auto-Refresh** - Keep data fresh automatically (daily/weekly/monthly)
3. **Opportunity Alerts** - Get notified of ranking opportunities
4. **Content Gap Analysis** - Know exactly what to write next
5. **Workflow Sync** - Auto-push to Notion/Asana/etc.

---

## üöÄ Quick Start (5 Minutes)

### Step 1: Install Dependencies

```bash
# Already done if you pulled latest code
pip install apscheduler==3.10.4
```

### Step 2: Test Autonomous Seed Discovery

```bash
# Auto-discover seeds from any website
python automation/seed_discoverer.py \
  --url "https://yourwebsite.com" \
  --niche "your industry"

# Example output:
# ‚úÖ DISCOVERED 50+ SEEDS:
#  1. seo tools
#  2. keyword research
#  3. backlink checker
#  ... and 47 more
```

### Step 3: Enable Scheduled Refresh

```python
# In Python
from automation.scheduler import setup_default_schedules

# Setup weekly refresh for project 1
setup_default_schedules(project_id=1, frequency="weekly")

# That's it! Your project will auto-refresh every week
```

### Step 4: Scan for Opportunities

```bash
# Scan project for opportunities
python automation/alert_engine.py 1

# Output:
# üéØ OPPORTUNITY ALERTS
# 1. üî¥ EASY_OPPORTUNITY
#    Keyword: keyword research tool
#    Action: Low competition, high value - target now!
```

---

## üìñ Detailed Setup

### 1. Autonomous Seed Discovery

**Use Case:** Don't know what keywords to target? Let the system discover them automatically.

**How it works:**
- Crawls your website and extracts topics
- Analyzes competitor websites
- Generates niche-specific keywords
- Recommends top 30 seeds

**Usage:**

```python
from automation.seed_discoverer import AutonomousSeedDiscoverer

discoverer = AutonomousSeedDiscoverer()

seeds = discoverer.discover_all(
    business_url="https://yourwebsite.com",
    business_description="SaaS for project management",
    competitors=["asana.com", "monday.com"],
    niche="project management software"
)

# Use recommended seeds
recommended_seeds = seeds['recommended']  # Top 30

# Or get seeds from specific sources
from_website = seeds['from_website']
from_competitors = seeds['from_competitors']
```

**CLI Usage:**

```bash
# Discover seeds
python automation/seed_discoverer.py \
  --url "https://ahrefs.com" \
  --description "SEO tools platform" \
  --competitors "semrush.com,moz.com" \
  --niche "seo software"

# Then use in your research
python cli.py create \
  --name "Auto-Discovered Project" \
  --seeds "$(python automation/seed_discoverer.py --url ahrefs.com | grep -A 50 'RECOMMENDED' | tail -30)" \
  --geo US
```

---

### 2. Scheduled Auto-Refresh

**Use Case:** SERPs change weekly. Keep your data fresh without manual work.

**How it works:**
- Runs on schedule (daily/weekly/monthly)
- Re-fetches SERP data
- Re-calculates difficulty scores
- Updates opportunity rankings
- Detects SERP changes

**Setup:**

```python
from automation.scheduler import get_scheduler, setup_default_schedules

# Option A: Use defaults (weekly refresh + daily monitoring)
setup_default_schedules(project_id=1, frequency="weekly")

# Option B: Custom schedule
scheduler = get_scheduler()

# Weekly refresh every Monday at 2 AM
scheduler.schedule_project_refresh(project_id=1, frequency="weekly")

# Daily SERP monitoring (top 50 keywords)
scheduler.schedule_serp_monitoring(project_id=1, top_n=50)

# Daily trend monitoring
scheduler.schedule_trend_monitoring(project_id=1)

# List all scheduled jobs
jobs = scheduler.list_jobs()
for job in jobs:
    print(f"{job['id']}: Next run at {job['next_run']}")

# Cancel a job
scheduler.cancel_job("refresh_project_1")
```

**Frequency Options:**
- `"daily"` - Every day at 2 AM
- `"weekly"` - Every Monday at 2 AM
- `"biweekly"` - Every 2 weeks
- `"monthly"` - 1st of month at 2 AM
- Custom cron: `"0 3 * * *"` (3 AM daily)

**Keep Scheduler Running:**

The scheduler needs to run continuously. Add to your web app or run as separate service:

```python
# Add to web_app_enhanced.py
from automation.scheduler import get_scheduler, setup_default_schedules

# On app startup
scheduler = get_scheduler()

# Setup schedules for all projects
with get_db() as db:
    projects = db.query(Project).all()
    for project in projects:
        setup_default_schedules(project.id, frequency="weekly")
```

---

### 3. Opportunity Alerts

**Use Case:** Don't miss time-sensitive ranking opportunities.

**Detects:**
- Keywords that became easier to rank for
- SERP volatility (competitors dropped)
- Trending topics (rising search interest)
- New PAA questions
- Quick wins (low difficulty + high volume)

**Setup:**

```python
from automation.alert_engine import OpportunityAlertEngine

engine = OpportunityAlertEngine()

# Scan project for opportunities
alerts = engine.scan_project(project_id=1)

# Alerts are sorted by urgency
for alert in alerts[:5]:  # Top 5
    print(f"{alert['urgency'].upper()}: {alert['action']}")

# Send notifications
engine.send_alerts(
    alerts=alerts,
    channels=['email', 'slack', 'webhook'],
    recipients=['you@example.com']
)
```

**Configure Notification Channels:**

```bash
# .env file
ALERT_EMAIL_RECIPIENTS=you@example.com,team@example.com
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
ALERT_WEBHOOK_URL=https://yourdomain.com/webhook/alerts
```

**Integrate with Scheduler (Auto-Alerts):**

```python
from automation.scheduler import get_scheduler
from automation.alert_engine import OpportunityAlertEngine

def scan_and_alert(project_id):
    """Scan for opportunities and send alerts."""
    engine = OpportunityAlertEngine()
    alerts = engine.scan_project(project_id)

    if alerts:
        engine.send_alerts(alerts, channels=['email', 'slack'])

# Schedule daily opportunity scan
scheduler = get_scheduler()
scheduler.scheduler.add_job(
    func=scan_and_alert,
    trigger='cron',
    args=[1],  # project_id
    hour=8,  # 8 AM daily
    id='daily_alerts_1'
)
```

---

### 4. Content Gap Analysis

**Use Case:** Don't know what content to create? System tells you exactly what's missing.

**How it works:**
- Imports existing content from sitemap/GSC/URL
- Compares against keyword opportunities
- Identifies coverage gaps
- Recommends content to create

**Setup:**

```python
from automation.gap_analyzer import ContentGapAnalyzer

analyzer = ContentGapAnalyzer()

# Option A: Auto-import from sitemap
results = analyzer.analyze_gaps(project_id=1, auto_import=True)

# Option B: Provide existing content manually
existing_content = [
    {'url': 'https://...', 'title': 'Article 1'},
    {'url': 'https://...', 'title': 'Article 2'},
]

results = analyzer.analyze_gaps(
    project_id=1,
    existing_content=existing_content,
    auto_import=False
)

# View results
print(f"Coverage: {results['coverage_score']:.1f}%")
print(f"Gaps Found: {len(results['gaps'])}")

for gap in results['gaps'][:10]:
    print(f"- {gap.get('topic') or gap.get('page_group')}")
    print(f"  Priority: {gap['priority']}/100")
    print(f"  Action: {gap['recommended_action']}")

# Get recommendations
for rec in results['recommendations']:
    print(f"\n{rec['recommendation']}")
    print(f"Impact: {rec['expected_impact']}, Timeline: {rec['timeline']}")
```

**CLI Usage:**

```bash
# Analyze gaps
python automation/gap_analyzer.py 1

# Output:
# üìä CONTENT GAP ANALYSIS
# Coverage Score: 45.2%
# Topics: 8/15 covered
#
# üéØ TOP 10 GAPS:
# 1. MISSING_TOPIC
#    Topic: keyword research tools
#    Priority: 87/100
#    Action: create_pillar_content
```

---

### 5. Workflow Sync (Notion Integration)

**Use Case:** Auto-create tasks in your project management tool.

**Platforms Supported:**
- ‚úÖ Notion (fully implemented)
- ‚è≥ Asana (coming soon)
- ‚è≥ Airtable (coming soon)

**Setup Notion:**

**Step 1: Get Notion API Key**

1. Go to https://www.notion.so/my-integrations
2. Click "New integration"
3. Name it "Keyword Research Automation"
4. Copy the "Internal Integration Token"

**Step 2: Create Notion Database**

1. Create a new database in Notion
2. Add these properties:
   - `Title` (title) - auto-created
   - `Status` (select): To Do, In Progress, Done
   - `Priority` (select): High, Medium, Low
   - `Word Count` (number)
   - `Opportunity Score` (number)
   - `Search Volume` (number)

3. Share database with your integration:
   - Click "..." on database
   - Click "Add connections"
   - Select your integration

4. Get database ID:
   - Open database as full page
   - Copy ID from URL: `notion.so/DATABASE_ID?v=...`

**Step 3: Configure .env**

```bash
# Add to .env
NOTION_API_KEY=secret_xxxxxxxxxxxxx
NOTION_DATABASE_ID=1234567890abcdef1234567890abcdef
```

**Step 4: Sync Project**

```python
from automation.workflow_sync import WorkflowSync

sync = WorkflowSync()

# Sync project 1 to Notion
results = sync.sync_to_notion(project_id=1, create_tasks=True)

if results['success']:
    print(f"‚úÖ Created {results['pages_created']} pages in Notion")
else:
    print(f"‚ùå Error: {results['error']}")
```

**CLI Usage:**

```bash
# Sync to Notion
python automation/workflow_sync.py 1

# Output:
# ‚úÖ Success! Created 25 pages in Notion
```

**Auto-Sync After Research:**

Add to your research pipeline:

```python
# In cli.py or orchestrator.py
from automation.workflow_sync import WorkflowSync

# After research completes
project_id = orchestrator.run_full_pipeline(...)

# Auto-sync to Notion
sync = WorkflowSync()
sync.sync_to_notion(project_id, create_tasks=True)
```

---

## üîß Advanced Configuration

### Environment Variables

Add these to your `.env` file for full automation:

```bash
# Scheduling
AUTO_REFRESH_FREQUENCY=weekly  # daily, weekly, biweekly, monthly
SERP_MONITORING_ENABLED=true
TREND_MONITORING_ENABLED=true

# Alerts
ALERT_EMAIL_RECIPIENTS=you@example.com
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/XXX
ALERT_WEBHOOK_URL=https://yourdomain.com/webhook

# Notion Integration
NOTION_API_KEY=secret_xxxxx
NOTION_DATABASE_ID=xxxxx

# Workflow
AUTO_SYNC_TO_NOTION=true  # Sync after each research run
AUTO_SEND_ALERTS=true     # Send alerts automatically
```

### Running Scheduler as Service

**Option A: Within Web App**

Already integrated! The scheduler starts when you run:

```bash
./start_dashboard.sh
```

**Option B: Standalone Service (Systemd)**

Create `/etc/systemd/system/keyword-scheduler.service`:

```ini
[Unit]
Description=Keyword Research Scheduler
After=network.target

[Service]
Type=simple
User=youruser
WorkingDirectory=/path/to/cursorkeyword
ExecStart=/path/to/cursorkeyword/venv/bin/python automation/scheduler.py
Restart=always

[Install]
WantedBy=multi-user.target
```

Start service:

```bash
sudo systemctl enable keyword-scheduler
sudo systemctl start keyword-scheduler
sudo systemctl status keyword-scheduler
```

---

## üìä Complete Automation Workflow

Here's a complete example of fully automated workflow:

```python
# complete_automation_example.py

from automation.seed_discoverer import AutonomousSeedDiscoverer
from automation.scheduler import get_scheduler, setup_default_schedules
from automation.alert_engine import OpportunityAlertEngine
from automation.gap_analyzer import ContentGapAnalyzer
from automation.workflow_sync import WorkflowSync
from orchestrator import KeywordResearchOrchestrator

# Step 1: Auto-discover seeds
discoverer = AutonomousSeedDiscoverer()
seeds_data = discoverer.discover_all(
    business_url="https://yoursite.com",
    competitors=["competitor1.com", "competitor2.com"],
    niche="your niche"
)

seeds = seeds_data['recommended'][:20]  # Top 20

# Step 2: Run research
orchestrator = KeywordResearchOrchestrator()
project_id = orchestrator.run_full_pipeline(
    project_name="Auto-Generated Project",
    seeds=seeds,
    geo="US",
    language="en",
    content_focus="informational"
)

# Step 3: Setup automation schedules
setup_default_schedules(project_id, frequency="weekly")

# Step 4: Scan for immediate opportunities
alert_engine = OpportunityAlertEngine()
alerts = alert_engine.scan_project(project_id)
alert_engine.send_alerts(alerts, channels=['email', 'slack'])

# Step 5: Analyze content gaps
gap_analyzer = ContentGapAnalyzer()
gap_results = gap_analyzer.analyze_gaps(project_id, auto_import=True)

print(f"Coverage: {gap_results['coverage_score']:.1f}%")
print(f"Gaps: {len(gap_results['gaps'])}")

# Step 6: Sync to Notion
workflow_sync = WorkflowSync()
sync_results = workflow_sync.sync_to_notion(project_id)

print(f"Created {sync_results['pages_created']} Notion tasks")

print("\n‚úÖ FULLY AUTOMATED!")
print("Your project will now:")
print("  - Auto-refresh weekly")
print("  - Monitor SERPs daily")
print("  - Send opportunity alerts")
print("  - Keep Notion in sync")
```

---

## üêõ Troubleshooting

### Scheduler Not Running

**Issue:** Jobs not executing

**Fix:**
```python
from automation.scheduler import get_scheduler

scheduler = get_scheduler()

# Check if scheduler is running
print(scheduler.scheduler.running)  # Should be True

# List jobs
jobs = scheduler.list_jobs()
print(f"Scheduled jobs: {len(jobs)}")

# Restart scheduler
scheduler.scheduler.shutdown()
scheduler = get_scheduler()  # Creates new instance
```

### Notion Sync Failing

**Issue:** `notion-client not installed`

**Fix:**
```bash
pip install notion-client==2.2.1
```

**Issue:** `Database ID missing`

**Fix:**
```bash
# Add to .env
NOTION_DATABASE_ID=your_database_id_here
```

### Alerts Not Sending

**Issue:** Email alerts not working

**Fix:** Email requires SMTP configuration (not yet implemented). Use Slack or webhook instead.

**Slack Setup:**
```bash
# .env
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK
```

---

## üìà What's Next

You now have **80% automated keyword research**!

### Current Automation:
- ‚úÖ Seed discovery
- ‚úÖ Scheduled refresh
- ‚úÖ Opportunity alerts
- ‚úÖ Gap analysis
- ‚úÖ Notion sync

### Coming in Tier 2 (if you want):
- üß† Multi-agent parallel processing (5-10x faster)
- üß† ML difficulty prediction
- üß† LLM-enhanced briefs
- üß† Natural language interface

### Need Help?

Run the demo:
```bash
python demo_automation.py
```

Test each component:
```bash
python automation/seed_discoverer.py --help
python automation/alert_engine.py 1
python automation/gap_analyzer.py 1
python automation/workflow_sync.py 1
```

---

**Congratulations! Your keyword research is now 80% automated.**

Save 10-15 hours per month. Never miss an opportunity again.
