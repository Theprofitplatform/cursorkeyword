version: '3.8'

services:
  # Main application service
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: keyword_research_app
    environment:
      # Database
      - DATABASE_URL=postgresql://keyword_user:keyword_pass@postgres:5432/keyword_research
      # Redis cache
      - REDIS_URL=redis://redis:6379/0
      # Load other env vars from .env file
      - SERPAPI_API_KEY=${SERPAPI_API_KEY}
      - GOOGLE_ADS_DEVELOPER_TOKEN=${GOOGLE_ADS_DEVELOPER_TOKEN:-}
      - NOTION_API_KEY=${NOTION_API_KEY:-}
      # Rate limits
      - SERP_API_RPM=${SERP_API_RPM:-20}
      - AUTOSUGGEST_RPM=${AUTOSUGGEST_RPM:-10}
      - TRENDS_RPM=${TRENDS_RPM:-60}
      # Clustering thresholds
      - TOPIC_CLUSTER_THRESHOLD=${TOPIC_CLUSTER_THRESHOLD:-0.78}
      - PAGE_GROUP_THRESHOLD=${PAGE_GROUP_THRESHOLD:-0.88}
      # Telemetry
      - ENABLE_TELEMETRY=${ENABLE_TELEMETRY:-false}
      - ENABLE_AUDIT_LOG=${ENABLE_AUDIT_LOG:-true}
    volumes:
      # Persist data directory
      - ./data:/workspace/data
      # Mount credentials
      - ./credentials:/workspace/credentials:ro
      # Development: mount source code
      - .:/workspace
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - keyword_network
    command: python cli.py serve --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    restart: unless-stopped

  # PostgreSQL database (production)
  postgres:
    image: postgres:16-alpine
    container_name: keyword_research_db
    environment:
      - POSTGRES_USER=keyword_user
      - POSTGRES_PASSWORD=keyword_pass
      - POSTGRES_DB=keyword_research
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init.sql:ro
    networks:
      - keyword_network
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U keyword_user -d keyword_research"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Redis cache and task queue
  redis:
    image: redis:7-alpine
    container_name: keyword_research_cache
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    networks:
      - keyword_network
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Celery worker (for async tasks)
  celery_worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: keyword_research_worker
    environment:
      - DATABASE_URL=postgresql://keyword_user:keyword_pass@postgres:5432/keyword_research
      - REDIS_URL=redis://redis:6379/0
      - SERPAPI_API_KEY=${SERPAPI_API_KEY}
      - CELERY_BROKER_URL=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
    volumes:
      - ./data:/workspace/data
      - ./credentials:/workspace/credentials:ro
      - .:/workspace
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - keyword_network
    command: celery -A orchestrator.celery worker --loglevel=info --concurrency=4
    restart: unless-stopped

  # Celery beat (for scheduled tasks)
  celery_beat:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: keyword_research_scheduler
    environment:
      - DATABASE_URL=postgresql://keyword_user:keyword_pass@postgres:5432/keyword_research
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
    volumes:
      - ./data:/workspace/data
      - .:/workspace
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - keyword_network
    command: celery -A orchestrator.celery beat --loglevel=info
    restart: unless-stopped

  # Flower (Celery monitoring UI) - optional
  flower:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: keyword_research_monitor
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
      - FLOWER_PORT=5555
    depends_on:
      - redis
      - celery_worker
    networks:
      - keyword_network
    ports:
      - "5555:5555"
    command: celery -A orchestrator.celery flower --port=5555
    restart: unless-stopped
    profiles:
      - monitoring

networks:
  keyword_network:
    driver: bridge

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local

# Usage Examples:
#
# Development (SQLite + no workers):
#   docker compose up app
#
# Production (all services):
#   docker compose up -d
#
# With monitoring:
#   docker compose --profile monitoring up -d
#
# CLI commands:
#   docker compose exec app python cli.py create --name "Test"
#   docker compose exec app python cli.py report 1
#   docker compose exec app python cli.py export 1 --format csv
#
# View logs:
#   docker compose logs -f app
#   docker compose logs -f celery_worker
#
# Database backup:
#   docker compose exec postgres pg_dump -U keyword_user keyword_research > backup.sql
#
# Database restore:
#   docker compose exec -T postgres psql -U keyword_user keyword_research < backup.sql
#
# Redis CLI:
#   docker compose exec redis redis-cli
#
# Stop all:
#   docker compose down
#
# Stop and remove volumes:
#   docker compose down -v
